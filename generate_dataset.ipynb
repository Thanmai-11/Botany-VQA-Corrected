{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Image-Grounded Botany-VQA Dataset Generation\n",
                "\n",
                "This notebook generates a corrected, image-grounded VQA dataset using vision-language models.\n",
                "\n",
                "## Steps:\n",
                "1. Setup and install dependencies\n",
                "2. Download Oxford Flowers 102 dataset\n",
                "3. Load VLM model (BLIP-2)\n",
                "4. Generate pilot dataset (100 images)\n",
                "5. Validate and review\n",
                "6. Generate full dataset (8,189 images)\n",
                "7. Final validation and statistics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q torch torchvision transformers pillow pandas numpy opencv-python scikit-learn tqdm matplotlib\n",
                "\n",
                "print(\"✓ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import os\n",
                "import json\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import torch\n",
                "\n",
                "# Import our modules\n",
                "from dataset_generator import BotanyVQAGenerator\n",
                "from question_templates import QuestionGenerator\n",
                "from visual_feature_extractor import VisualFeatureExtractor\n",
                "from vqa_validator import VQAValidator\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Download Oxford Flowers 102 Dataset\n",
                "\n",
                "Run these commands in terminal or use the cells below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download dataset (uncomment if needed)\n",
                "# !wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
                "# !tar -xzf 102flowers.tgz\n",
                "\n",
                "# Download labels\n",
                "# !wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
                "\n",
                "# Download category names (you may need to create this manually)\n",
                "# See README.md for instructions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify dataset structure\n",
                "IMAGE_DIR = \"oxford_flowers_102/jpg\"  # Update this path\n",
                "LABELS_FILE = \"oxford_flowers_102/labels.json\"  # Update this path\n",
                "\n",
                "if os.path.exists(IMAGE_DIR):\n",
                "    num_images = len([f for f in os.listdir(IMAGE_DIR) if f.endswith('.jpg')])\n",
                "    print(f\"✓ Found {num_images} images in {IMAGE_DIR}\")\n",
                "else:\n",
                "    print(f\"✗ Image directory not found: {IMAGE_DIR}\")\n",
                "\n",
                "if os.path.exists(LABELS_FILE):\n",
                "    with open(LABELS_FILE, 'r') as f:\n",
                "        labels = json.load(f)\n",
                "    print(f\"✓ Found {len(labels)} labels in {LABELS_FILE}\")\n",
                "else:\n",
                "    print(f\"✗ Labels file not found: {LABELS_FILE}\")\n",
                "    print(\"Run create_labels.py to generate labels.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load VLM Model (BLIP-2)\n",
                "\n",
                "This will download the model (~5GB). First time may take a few minutes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize generator\n",
                "generator = BotanyVQAGenerator(\n",
                "    model_name=\"Salesforce/blip2-opt-2.7b\",  # You can change to blip2-flan-t5-xl for better quality\n",
                "    device=None  # Auto-detect GPU/CPU\n",
                ")\n",
                "\n",
                "print(\"✓ Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Test on a Single Image\n",
                "\n",
                "Let's test the model on one image first:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a sample image\n",
                "sample_image_path = os.path.join(IMAGE_DIR, \"image_00001.jpg\")\n",
                "sample_image = Image.open(sample_image_path)\n",
                "\n",
                "# Display image\n",
                "plt.figure(figsize=(6, 6))\n",
                "plt.imshow(sample_image)\n",
                "plt.axis('off')\n",
                "plt.title(\"Sample Flower Image\")\n",
                "plt.show()\n",
                "\n",
                "# Ask a test question\n",
                "test_question = \"What type of flower is this?\"\n",
                "answer = generator.ask_question(sample_image_path, test_question)\n",
                "\n",
                "print(f\"\\nQuestion: {test_question}\")\n",
                "print(f\"Answer: {answer}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Generate Pilot Dataset (100 images)\n",
                "\n",
                "Let's generate a small pilot dataset first to validate quality:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate pilot dataset\n",
                "pilot_df = generator.generate_dataset(\n",
                "    image_dir=IMAGE_DIR,\n",
                "    labels_file=LABELS_FILE,\n",
                "    output_csv=\"botany_vqa_pilot.csv\",\n",
                "    num_images=100,  # Only 100 images for pilot\n",
                "    qa_per_image=10\n",
                ")\n",
                "\n",
                "print(f\"\\n✓ Pilot dataset generated!\")\n",
                "print(f\"Total QA pairs: {len(pilot_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Validate Pilot Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run validation\n",
                "validator = VQAValidator(pilot_df['flower_category'].unique().tolist())\n",
                "validation_results = validator.run_all_validations(pilot_df)\n",
                "\n",
                "# Print report\n",
                "report = validator.generate_validation_report(validation_results)\n",
                "print(report)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize question type distribution\n",
                "plt.figure(figsize=(10, 6))\n",
                "pilot_df['question_type'].value_counts().plot(kind='bar')\n",
                "plt.title('Question Type Distribution (Pilot)')\n",
                "plt.xlabel('Question Type')\n",
                "plt.ylabel('Count')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View sample QA pairs\n",
                "print(\"Sample QA pairs from pilot dataset:\\n\")\n",
                "sample_image = pilot_df['image_path'].iloc[0]\n",
                "sample_qa = pilot_df[pilot_df['image_path'] == sample_image]\n",
                "\n",
                "# Display image\n",
                "img = Image.open(os.path.join(IMAGE_DIR, sample_image))\n",
                "plt.figure(figsize=(6, 6))\n",
                "plt.imshow(img)\n",
                "plt.axis('off')\n",
                "plt.title(f\"Image: {sample_image}\")\n",
                "plt.show()\n",
                "\n",
                "# Display QA pairs\n",
                "for idx, row in sample_qa.iterrows():\n",
                "    print(f\"Q: {row['question']}\")\n",
                "    print(f\"A: {row['answer']}\")\n",
                "    print(f\"Type: {row['question_type']} | Level: {row['difficulty_level']}\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Generate Full Dataset (All 8,189 images)\n",
                "\n",
                "⚠️ **Warning**: This will take several hours depending on your GPU/CPU.\n",
                "\n",
                "Estimated time:\n",
                "- With GPU: 3-5 hours\n",
                "- With CPU: 10-15 hours"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate full dataset (uncomment when ready)\n",
                "# full_df = generator.generate_dataset(\n",
                "#     image_dir=IMAGE_DIR,\n",
                "#     labels_file=LABELS_FILE,\n",
                "#     output_csv=\"botany_vqa_grounded.csv\",\n",
                "#     num_images=None,  # Process all images\n",
                "#     qa_per_image=10\n",
                "# )\n",
                "\n",
                "# print(f\"\\n✓ Full dataset generated!\")\n",
                "# print(f\"Total QA pairs: {len(full_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Final Validation and Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load generated dataset\n",
                "df = pd.read_csv(\"botany_vqa_grounded.csv\")  # or botany_vqa_pilot.csv\n",
                "\n",
                "# Generate statistics\n",
                "generator.generate_statistics(df, \"dataset_statistics.json\")\n",
                "\n",
                "# Run final validation\n",
                "validator = VQAValidator(df['flower_category'].unique().tolist())\n",
                "validation_results = validator.run_all_validations(df)\n",
                "report = validator.generate_validation_report(validation_results)\n",
                "\n",
                "print(report)\n",
                "\n",
                "# Save report\n",
                "with open(\"validation_report.txt\", \"w\") as f:\n",
                "    f.write(report)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Visualize Dataset Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load statistics\n",
                "with open(\"dataset_statistics.json\", \"r\") as f:\n",
                "    stats = json.load(f)\n",
                "\n",
                "print(\"Dataset Statistics:\")\n",
                "print(json.dumps(stats, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create visualizations\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
                "\n",
                "# Question type distribution\n",
                "ax1 = axes[0, 0]\n",
                "df['question_type'].value_counts().plot(kind='bar', ax=ax1)\n",
                "ax1.set_title('Question Type Distribution')\n",
                "ax1.set_xlabel('Question Type')\n",
                "ax1.set_ylabel('Count')\n",
                "ax1.tick_params(axis='x', rotation=45)\n",
                "\n",
                "# Difficulty level distribution\n",
                "ax2 = axes[0, 1]\n",
                "df['difficulty_level'].value_counts().sort_index().plot(kind='bar', ax=ax2)\n",
                "ax2.set_title('Difficulty Level Distribution')\n",
                "ax2.set_xlabel('Difficulty Level')\n",
                "ax2.set_ylabel('Count')\n",
                "\n",
                "# Answer length distribution\n",
                "ax3 = axes[1, 0]\n",
                "df['answer'].str.len().hist(bins=30, ax=ax3)\n",
                "ax3.set_title('Answer Length Distribution')\n",
                "ax3.set_xlabel('Answer Length (characters)')\n",
                "ax3.set_ylabel('Frequency')\n",
                "\n",
                "# Top 10 flower categories\n",
                "ax4 = axes[1, 1]\n",
                "df['flower_category'].value_counts().head(10).plot(kind='barh', ax=ax4)\n",
                "ax4.set_title('Top 10 Flower Categories')\n",
                "ax4.set_xlabel('Count')\n",
                "ax4.set_ylabel('Flower Category')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('dataset_statistics.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"✓ Visualizations saved to dataset_statistics.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Compare with Original Erroneous Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load original dataset\n",
                "original_df = pd.read_csv(\"https://raw.githubusercontent.com/Thanmai-11/Botany-VQA/refs/heads/main/botany_vqa_v1.csv\")\n",
                "\n",
                "# Compare same image\n",
                "test_image = \"jpg/image_00001.jpg\"\n",
                "\n",
                "print(\"ORIGINAL DATASET (Erroneous):\")\n",
                "print(\"=\" * 60)\n",
                "original_qa = original_df[original_df['image_path'] == test_image]\n",
                "for idx, row in original_qa.head(5).iterrows():\n",
                "    print(f\"Q: {row['question']}\")\n",
                "    print(f\"A: {row['answer']}\")\n",
                "    print()\n",
                "\n",
                "print(\"\\nCORRECTED DATASET (Image-Grounded):\")\n",
                "print(\"=\" * 60)\n",
                "corrected_qa = df[df['image_path'] == test_image]\n",
                "for idx, row in corrected_qa.head(5).iterrows():\n",
                "    print(f\"Q: {row['question']}\")\n",
                "    print(f\"A: {row['answer']}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ✅ Done!\n",
                "\n",
                "Your image-grounded Botany-VQA dataset is ready!\n",
                "\n",
                "**Next steps:**\n",
                "1. Review the validation report\n",
                "2. Manually inspect sample QA pairs\n",
                "3. Use the dataset for your VQA research\n",
                "4. Publish the corrected dataset on GitHub\n",
                "5. Update your research paper with the new dataset"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}